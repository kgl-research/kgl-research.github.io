{\rtf1\ansi\ansicpg1252\cocoartf2638
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fmodern\fcharset0 Courier;\f1\fswiss\fcharset0 Helvetica-Oblique;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs26 \cf0 \expnd0\expndtw0\kerning0
---\
layout: paper\
title: Similarities and Differences in Human vs. Computational Representations of Non-semantic Inspirational Design Stimuli
\f1\i\fs20 \kerning1\expnd0\expndtw0 \

\f0\i0\fs26 \expnd0\expndtw0\kerning0
image: /images/papers/2023-IDETC-kwon-humancompsim.png\
authors: Kwon E and Goucher-Lambert K.\
year: 2023\
ref: Kwon and Goucher-Lambert. <i>ASME IDETC</i> 2023\
journal: Proceedings of the ASME International Design Engineering Technical Conferences (2023).\
pdf: /pdfs/papers/kwon-comphumanstimuli-idetc.pdf\
doi: \
---\
		\
\
# Abstract	\
\
As inspirational stimuli can assist designers with achieving\
enhanced design outcomes, supporting the retrieval of impactful\
sources of inspiration is important. Existing methods facilitating\
this retrieval have relied mostly on semantic relationships, e.g.,\
analogical distances. Increasingly, data-driven methods can be\
leveraged to represent diverse stimuli in terms of multi-modal information,\
enabling designers to access stimuli in terms of less explored,\
non-text-based relationships. Toward improved retrieval\
of multi-modal representations of inspirational stimuli, this work\
compares human-evaluated and computationally derived similarities\
between stimuli in terms of non-text-based visual and functional\
features. A human subjects study (n=36) was conducted\
where similarity assessments between triplets of 3D-model parts\
were collected and used to construct psychological embedding\
spaces. Distances between unique part embeddings were used to\
represent similarities in terms of visual and functional features.\
Obtained distances were compared with computed distances between\
embeddings of the same stimuli generated using artificial\
intelligence (AI)-based deep-learning approaches. When used to\
assess similarity in appearance and function, these representations\
were found to be largely consistent, with highest agreement\
found when assessing pairs of stimuli with low similarity. Alignment\
between models was otherwise lower when identifying the\
same pairs of stimuli with higher levels of similarity. Importantly,\
qualitative data also revealed insights regarding how humans\
made similarity assessments, including more abstract information\
not captured using AI-based approaches. Toward providing\
inspiration to designers that considers design problems, ideas,\
and solutions in terms of non-text-based relationships, further\
exploration of how these relationships are represented and evaluated\
is encouraged.}